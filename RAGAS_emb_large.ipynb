{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Section 1: Imports\n",
    "import os\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import random\n",
    "import pypdf\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Retrievers\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import (\n",
    "    MultiQueryRetriever,\n",
    "    ContextualCompressionRetriever,\n",
    "    EnsembleRetriever,\n",
    "    ParentDocumentRetriever\n",
    ")\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Ragas (0.2.10) \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ContextPrecision, ContextRecall, AnswerRelevancy, ContextEntityRecall\n",
    "from ragas.testset import TestsetGenerator\n",
    "from datasets import Dataset  # For Ragas 0.2.10 evaluation format\n",
    "\n",
    "# LangSmith\n",
    "from langsmith import Client\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "# Section 1: Imports\n",
    "import os\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import random\n",
    "import pypdf\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Retrievers\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import (\n",
    "    MultiQueryRetriever,\n",
    "    ContextualCompressionRetriever,\n",
    "    EnsembleRetriever,\n",
    "    ParentDocumentRetriever\n",
    ")\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Ragas (0.2.10) - \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ContextPrecision, ContextRecall, AnswerRelevancy, ContextEntityRecall\n",
    "from ragas.testset import TestsetGenerator\n",
    "from datasets import Dataset  # For Ragas 0.2.10 evaluation format\n",
    "\n",
    "# LangSmith\n",
    "from langsmith import Client\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys configured and LangSmith tracing enabled\n"
     ]
    }
   ],
   "source": [
    "# API Keys Setup\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass(\"Enter Cohere API Key: \")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Enter LangChain/LangSmith API Key: \")\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Advanced_Retrieval_Evaluation\"\n",
    "\n",
    "langsmith_client = Client()\n",
    "print(\"‚úÖ API keys configured and LangSmith tracing enabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading PDF: data/TC.pdf\n",
      "‚úÖ Loaded 6 pages from PDF\n",
      "   First page preview: Traffic Congestion and Reliability: Trends, \n",
      "Causes, Measurement, and Strategic \n",
      "Approaches \n",
      "Traffic congestion is a pervasive issue that impacts economic efficiency, quality of life, and \n",
      "overall tra...\n",
      "‚úÖ Split into 18 chunks for retrieval\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Load PDF\n",
    "pdf_path = \"data/TC.pdf\"\n",
    "print(f\"üìÑ Loading PDF: {pdf_path}\")\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} pages from PDF\")\n",
    "print(f\"   First page preview: {documents[0].page_content[:200]}...\")\n",
    "\n",
    "# Split for retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"‚úÖ Split into {len(chunks)} chunks for retrieval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220ca587dbac49dc915c79bc320da9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d929a2164d6a4c79b8ad9f368663ef8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41360e3aea74eada221d3c84b459693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71e95f33afd4e9a990079dc5558bce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dff6df18ed4940abdff79fd05187ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7dc60b67f54288afedf5f23e56a847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c0d04eceb74f52bea33915effd7f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 10 synthetic questions!\n",
      "\\nüìã Sample questions:\n",
      "   1. What traffic congestion mean?\n",
      "   2. How do special events impact traffic demand and congestion?\n",
      "   3. What role does ITS play in monitoring traffic congestion and improving roadway efficiency?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What traffic congestion mean?</td>\n",
       "      <td>[Traffic Congestion and Reliability: Trends, \\...</td>\n",
       "      <td>Traffic congestion is when too many vehicles a...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do special events impact traffic demand an...</td>\n",
       "      <td>[speeds and increasing delays. These disruptio...</td>\n",
       "      <td>Special events, such as major sporting contest...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What role does ITS play in monitoring traffic ...</td>\n",
       "      <td>[Travel time is the fundamental metric for con...</td>\n",
       "      <td>ITS sensors are one of the data sources used t...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does congestion impact households and the ...</td>\n",
       "      <td>[Households face both financial and time-relat...</td>\n",
       "      <td>Congestion affects households by imposing fina...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does traffic congestion management contrib...</td>\n",
       "      <td>[efficiently not only restores traffic flow mo...</td>\n",
       "      <td>Traffic congestion management contributes to l...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the primary causes of traffic congest...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic congestion is primarily caused by a co...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the primary causes of traffic congest...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic congestion is primarily caused by a co...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do traffic incidents contribute to traffic...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic incidents, such as crashes, breakdowns...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the causes of traffic congestion and ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic congestion is caused by multiple inter...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the causes of traffic congestion and ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic congestion is caused by multiple inter...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                      What traffic congestion mean?   \n",
       "1  How do special events impact traffic demand an...   \n",
       "2  What role does ITS play in monitoring traffic ...   \n",
       "3  How does congestion impact households and the ...   \n",
       "4  How does traffic congestion management contrib...   \n",
       "5  What are the primary causes of traffic congest...   \n",
       "6  What are the primary causes of traffic congest...   \n",
       "7  How do traffic incidents contribute to traffic...   \n",
       "8  What are the causes of traffic congestion and ...   \n",
       "9  What are the causes of traffic congestion and ...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Traffic Congestion and Reliability: Trends, \\...   \n",
       "1  [speeds and increasing delays. These disruptio...   \n",
       "2  [Travel time is the fundamental metric for con...   \n",
       "3  [Households face both financial and time-relat...   \n",
       "4  [efficiently not only restores traffic flow mo...   \n",
       "5  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "6  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "7  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "8  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "9  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Traffic congestion is when too many vehicles a...   \n",
       "1  Special events, such as major sporting contest...   \n",
       "2  ITS sensors are one of the data sources used t...   \n",
       "3  Congestion affects households by imposing fina...   \n",
       "4  Traffic congestion management contributes to l...   \n",
       "5  Traffic congestion is primarily caused by a co...   \n",
       "6  Traffic congestion is primarily caused by a co...   \n",
       "7  Traffic incidents, such as crashes, breakdowns...   \n",
       "8  Traffic congestion is caused by multiple inter...   \n",
       "9  Traffic congestion is caused by multiple inter...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_specific_query_synthesizer  \n",
       "7  multi_hop_specific_query_synthesizer  \n",
       "8  multi_hop_specific_query_synthesizer  \n",
       "9  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3: Golden Dataset Generation (Ragas SDG 0.2.10) - 10 Questions\n",
    "\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "golden_dataset = generator.generate_with_langchain_docs(documents, testset_size=10)\n",
    "\n",
    "# Convert to pandas for easy access\n",
    "golden_df = golden_dataset.to_pandas()\n",
    "\n",
    "print(f\"‚úÖ Generated {len(golden_df)} synthetic questions!\")\n",
    "print(\"\\\\nüìã Sample questions:\")\n",
    "for i in range(min(3, len(golden_df))):\n",
    "    print(f\"   {i+1}. {golden_df.iloc[i]['user_input']}\")\n",
    "\n",
    "# Display full dataset\n",
    "golden_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All 6 retrievers ready\n"
     ]
    }
   ],
   "source": [
    "# Section 4: Setup All 6 Retrievers\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 1. Naive\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "naive = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 2. BM25\n",
    "bm25 = BM25Retriever.from_documents(chunks); bm25.k = 5\n",
    "\n",
    "# 3. Multi-Query\n",
    "multi_query = MultiQueryRetriever.from_llm(retriever=naive, llm=llm)\n",
    "\n",
    "# 4. Parent Document\n",
    "store = InMemoryStore()\n",
    "parent_doc = ParentDocumentRetriever(\n",
    "    vectorstore=FAISS.from_documents(RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50).split_documents(documents), embeddings),\n",
    "    docstore=store,\n",
    "    child_splitter=RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50),\n",
    "    parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    ")\n",
    "parent_doc.add_documents(documents)\n",
    "\n",
    "# 5. Contextual Compression (Cohere Rerank)\n",
    "compression = ContextualCompressionRetriever(\n",
    "    base_compressor=CohereRerank(model=\"rerank-english-v3.0\", top_n=5),\n",
    "    base_retriever=naive\n",
    ")\n",
    "\n",
    "# 6. Ensemble\n",
    "ensemble = EnsembleRetriever(retrievers=[bm25, naive, multi_query], weights=[0.3, 0.4, 0.3])\n",
    "\n",
    "retrievers = {\"Naive\": naive, \"BM25\": bm25, \"Multi-Query\": multi_query, \n",
    "              \"Parent Document\": parent_doc, \"Contextual Compression\": compression, \"Ensemble\": ensemble}\n",
    "\n",
    "print(f\"‚úÖ All {len(retrievers)} retrievers ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 6 RAG chains ready with LangSmith tracking\n"
     ]
    }
   ],
   "source": [
    "# Section 5: Build RAG Chains with LangSmith Tracking\n",
    "RAG_TEMPLATE = \"\"\"You are a helpful AI assistant. Use the context to answer the question.\n",
    "If you don't know, say so. Don't make up information.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "rag_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def build_chain(retriever):\n",
    "    return (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=lambda x: \"\\\\n\\\\n\".join(d.page_content for d in x[\"context\"]))\n",
    "        | {\"response\": rag_prompt | rag_llm, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "\n",
    "rag_chains = {name: build_chain(r) for name, r in retrievers.items()}\n",
    "print(f\"‚úÖ {len(rag_chains)} RAG chains ready with LangSmith tracking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating 6 retrievers with 5 Ragas metrics...\n",
      "\\n==================================================\\nEvaluating: Naive\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480b3ce52ecc4428ac4da0c6e981e97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DEBUG: Type = <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "  DEBUG: ragas_results = {'context_precision': 1.0000, 'context_recall': 0.9750, 'answer_relevancy': 0.8517, 'faithfulness': 0.9640, 'context_entity_recall': 0.4849}\n",
      "  ‚úÖ Complete (155.6s): {}\n",
      "\\n==================================================\\nEvaluating: BM25\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce5c9abc30a47b4903e7f5aaa4b02fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DEBUG: Type = <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "  DEBUG: ragas_results = {'context_precision': 1.0000, 'context_recall': 0.9300, 'answer_relevancy': 0.8628, 'faithfulness': 0.9006, 'context_entity_recall': 0.2230}\n",
      "  ‚úÖ Complete (123.2s): {}\n",
      "\\n==================================================\\nEvaluating: Multi-Query\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c42fb4b1fb4df0a2b4e67c2e128ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DEBUG: Type = <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "  DEBUG: ragas_results = {'context_precision': 1.0000, 'context_recall': 1.0000, 'answer_relevancy': 0.8530, 'faithfulness': 0.9527, 'context_entity_recall': 0.5006}\n",
      "  ‚úÖ Complete (176.8s): {}\n",
      "\\n==================================================\\nEvaluating: Parent Document\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6388707451499786819112c01ddefb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DEBUG: Type = <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "  DEBUG: ragas_results = {'context_precision': 1.0000, 'context_recall': 0.9500, 'answer_relevancy': 0.8577, 'faithfulness': 0.9483, 'context_entity_recall': 0.1556}\n",
      "  ‚úÖ Complete (106.9s): {}\n",
      "\\n==================================================\\nEvaluating: Contextual Compression\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07463120a2824f3884bcc653be14ab67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DEBUG: Type = <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "  DEBUG: ragas_results = {'context_precision': 1.0000, 'context_recall': 0.9750, 'answer_relevancy': 0.8550, 'faithfulness': 0.9920, 'context_entity_recall': 0.5031}\n",
      "  ‚úÖ Complete (139.3s): {}\n",
      "\\n==================================================\\nEvaluating: Ensemble\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1946f80ba71648b5b50672280e785d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DEBUG: Type = <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "  DEBUG: ragas_results = {'context_precision': 1.0000, 'context_recall': 0.9500, 'answer_relevancy': 0.8552, 'faithfulness': 0.9644, 'context_entity_recall': 0.4081}\n",
      "  ‚úÖ Complete (179.9s): {}\n",
      "\\n======================================================================\n",
      "üìä EVALUATION COMPLETE - QUICK SUMMARY\n",
      "======================================================================\n",
      "             Retriever  Questions Latency (s)\n",
      "                 Naive         10       155.6\n",
      "                  BM25         10       123.2\n",
      "           Multi-Query         10       176.8\n",
      "       Parent Document         10       106.9\n",
      "Contextual Compression         10       139.3\n",
      "              Ensemble         10       179.9\n",
      "\\n‚úÖ Full detailed results in next cell...\n"
     ]
    }
   ],
   "source": [
    "# Section 6: Evaluate All Retrievers with Ragas Metrics (0.2.10 API)\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "# Initialize metrics with LLM and embeddings\n",
    "metrics = [\n",
    "    ContextPrecision(llm=evaluator_llm), \n",
    "    ContextRecall(llm=evaluator_llm), \n",
    "    AnswerRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings), \n",
    "    Faithfulness(llm=evaluator_llm), \n",
    "    ContextEntityRecall(llm=evaluator_llm)\n",
    "]\n",
    "\n",
    "print(f\"üìä Evaluating {len(retrievers)} retrievers with {len(metrics)} Ragas metrics...\")\n",
    "evaluation_results = {}\n",
    "\n",
    "for name, chain in rag_chains.items():\n",
    "    print(f\"\\\\n{'='*50}\\\\nEvaluating: {name}\\\\n{'='*50}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Build evaluation data (Ragas 0.2.10 format)\n",
    "    # Required columns: question, answer, contexts, ground_truths, reference\n",
    "    eval_data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truths\": [], \"reference\": []}\n",
    "    \n",
    "    # Use enumerate to get proper sequential count\n",
    "    for idx in range(len(golden_df)):\n",
    "        row = golden_df.iloc[idx]\n",
    "        print(f\"  Question {idx+1}/{len(golden_df)}...\", end=\"\\\\r\")\n",
    "        try:\n",
    "            question = row['user_input']\n",
    "            reference = row.get('reference', '')\n",
    "            \n",
    "            result = chain.invoke({\"question\": question})\n",
    "            \n",
    "            eval_data[\"question\"].append(question)\n",
    "            eval_data[\"answer\"].append(result[\"response\"].content)\n",
    "            eval_data[\"contexts\"].append([result[\"context\"]])\n",
    "            eval_data[\"ground_truths\"].append([reference] if reference else [question])\n",
    "            eval_data[\"reference\"].append(reference if reference else question)\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "        except Exception as e:\n",
    "            print(f\"\\\\n  ‚úó Error on question {idx+1}: {e}\")\n",
    "            continue  # Skip failed questions\n",
    "    \n",
    "    print(f\"\\\\n  Running Ragas evaluation...\")\n",
    "    from datasets import Dataset\n",
    "    eval_dataset = Dataset.from_dict(eval_data)\n",
    "    ragas_results = evaluate(eval_dataset, metrics=metrics, llm=evaluator_llm)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # DEBUG: Print the actual ragas_results to see its structure\n",
    "    print(f\"  DEBUG: Type = {type(ragas_results)}\")\n",
    "    print(f\"  DEBUG: ragas_results = {ragas_results}\")\n",
    "    \n",
    "    # Extract metric scores for display\n",
    "    metric_scores = {}\n",
    "    \n",
    "    # Debug: Check if it's a dict or object\n",
    "    if isinstance(ragas_results, dict):\n",
    "        metric_scores = {k: f\"{v:.4f}\" if isinstance(v, (int, float)) else str(v) for k, v in ragas_results.items()}\n",
    "    else:\n",
    "        # Try to get attributes\n",
    "        for metric in metrics:\n",
    "            if hasattr(ragas_results, metric.name):\n",
    "                value = getattr(ragas_results, metric.name)\n",
    "                metric_scores[metric.name] = f\"{value:.4f}\" if isinstance(value, (int, float)) else str(value)\n",
    "        \n",
    "        # If still empty, try all non-private attributes\n",
    "        if not metric_scores:\n",
    "            all_attrs = [a for a in dir(ragas_results) if not a.startswith('_')]\n",
    "            for attr in all_attrs:\n",
    "                try:\n",
    "                    value = getattr(ragas_results, attr)\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        metric_scores[attr] = f\"{value:.4f}\"\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    evaluation_results[name] = {\"ragas\": ragas_results, \"latency\": elapsed, \"count\": len(eval_data[\"question\"])}\n",
    "    print(f\"  ‚úÖ Complete ({elapsed:.1f}s): {metric_scores}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"üìä EVALUATION COMPLETE - QUICK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Quick summary table\n",
    "summary_data = []\n",
    "for name, res in evaluation_results.items():\n",
    "    ragas_result = res[\"ragas\"]\n",
    "    row = {\n",
    "        \"Retriever\": name,\n",
    "        \"Questions\": res[\"count\"],\n",
    "        \"Latency (s)\": f\"{res['latency']:.1f}\"\n",
    "    }\n",
    "    # Add first few metrics for quick view\n",
    "    for metric in metrics[:3]:  # Show first 3 metrics\n",
    "        if hasattr(ragas_result, metric.name):\n",
    "            row[metric.name] = f\"{getattr(ragas_result, metric.name):.3f}\"\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\\\n‚úÖ Full detailed results in next cell...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüìä FINAL RESULTS:\n",
      "             Retriever  Latency (s)  Questions\n",
      "                  BM25   110.129075         10\n",
      "                 Naive   122.944868         10\n",
      "       Parent Document   124.400105         10\n",
      "Contextual Compression   128.307050         10\n",
      "           Multi-Query   149.312663         10\n",
      "              Ensemble   173.492774         10\n",
      "\\n‚úÖ Saved to: retriever_evaluation_results.csv\n",
      "\\nüèÜ Best by Metric:\n",
      "\\nüìù RECOMMENDATION:\n",
      "\n",
      "Based on cost, latency, and performance:\n",
      "- For best performance: Check scores above\n",
      "- For best speed: Check latency column  \n",
      "- For balanced approach: Consider all factors\n",
      "\n",
      "\\nüîó LangSmith Links:\n",
      "   üìä Project Dashboard: https://smith.langchain.com/o/default/projects/p/advanced-retrieval-evaluation\n",
      "   üîç View All Traces: https://smith.langchain.com/\n",
      "   üìÅ Project: Advanced_Retrieval_Evaluation\n",
      "\\n   üí° Tip: Use the LangSmith dashboard to:\n",
      "      - View detailed traces for each retriever\n",
      "      - Compare latency and token usage\n",
      "      - Debug retrieval quality issues\n",
      "      - Analyze cost breakdowns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever</th>\n",
       "      <th>Latency (s)</th>\n",
       "      <th>Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25</td>\n",
       "      <td>110.129075</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive</td>\n",
       "      <td>122.944868</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parent Document</td>\n",
       "      <td>124.400105</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contextual Compression</td>\n",
       "      <td>128.307050</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-Query</td>\n",
       "      <td>149.312663</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>173.492774</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Retriever  Latency (s)  Questions\n",
       "1                    BM25   110.129075         10\n",
       "0                   Naive   122.944868         10\n",
       "3         Parent Document   124.400105         10\n",
       "4  Contextual Compression   128.307050         10\n",
       "2             Multi-Query   149.312663         10\n",
       "5                Ensemble   173.492774         10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 7: Results Compilation and Analysis\n",
    "results_data = []\n",
    "for name, res in evaluation_results.items():\n",
    "    # Ragas 0.2.10: EvaluationResult has metric scores as attributes\n",
    "    ragas_result = res[\"ragas\"]\n",
    "    row = {\n",
    "        \"Retriever\": name, \n",
    "        \"Latency (s)\": res[\"latency\"], \n",
    "        \"Questions\": res[\"count\"]\n",
    "    }\n",
    "    \n",
    "    # Extract metric scores from EvaluationResult\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.name\n",
    "        if hasattr(ragas_result, metric_name):\n",
    "            row[metric_name] = getattr(ragas_result, metric_name)\n",
    "    \n",
    "    results_data.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results_data).sort_values(\"Latency (s)\")\n",
    "print(\"\\\\nüìä FINAL RESULTS:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(\"retriever_evaluation_results.csv\", index=False)\n",
    "print(\"\\\\n‚úÖ Saved to: retriever_evaluation_results.csv\")\n",
    "\n",
    "# Analysis\n",
    "best = {col: results_df.loc[results_df[col].idxmax(), \"Retriever\"] \n",
    "        for col in results_df.columns if col not in [\"Retriever\", \"Latency (s)\", \"Questions\"]}\n",
    "\n",
    "print(\"\\\\nüèÜ Best by Metric:\")\n",
    "for metric, retriever in best.items():\n",
    "    print(f\"   {metric}: {retriever}\")\n",
    "\n",
    "print(\"\\\\nüìù RECOMMENDATION:\")\n",
    "print(\"\"\"\n",
    "Based on cost, latency, and performance:\n",
    "- For best performance: Check scores above\n",
    "- For best speed: Check latency column  \n",
    "- For balanced approach: Consider all factors\n",
    "\"\"\")\n",
    "\n",
    "# LangSmith URLs\n",
    "project_name = \"Advanced_Retrieval_Evaluation\"\n",
    "print(\"\\\\nüîó LangSmith Links:\")\n",
    "print(f\"   üìä Project Dashboard: https://smith.langchain.com/o/default/projects/p/{project_name.replace('_', '-').lower()}\")\n",
    "print(f\"   üîç View All Traces: https://smith.langchain.com/\")\n",
    "print(f\"   üìÅ Project: {project_name}\")\n",
    "print(\"\\\\n   üí° Tip: Use the LangSmith dashboard to:\")\n",
    "print(\"      - View detailed traces for each retriever\")\n",
    "print(\"      - Compare latency and token usage\")\n",
    "print(\"      - Debug retrieval quality issues\")\n",
    "print(\"      - Analyze cost breakdowns\")\n",
    "\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
