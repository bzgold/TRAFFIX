{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity #1: Advanced Retrieval Evaluation\n",
    "\n",
    "Evaluates 6 retriever methods using Ragas SDG and LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR CODE HERE\n",
    "# Section 1: Imports\n",
    "import os\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import random\n",
    "import pypdf\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Retrievers\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import (\n",
    "    MultiQueryRetriever,\n",
    "    ContextualCompressionRetriever,\n",
    "    EnsembleRetriever,\n",
    "    ParentDocumentRetriever\n",
    ")\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Ragas (0.2.10) \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ContextPrecision, ContextRecall, AnswerRelevancy, ContextEntityRecall\n",
    "from ragas.testset import TestsetGenerator\n",
    "from datasets import Dataset  # For Ragas 0.2.10 evaluation format\n",
    "\n",
    "# LangSmith\n",
    "from langsmith import Client\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "# Section 1: Imports\n",
    "import os\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import random\n",
    "import pypdf\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Retrievers\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import (\n",
    "    MultiQueryRetriever,\n",
    "    ContextualCompressionRetriever,\n",
    "    EnsembleRetriever,\n",
    "    ParentDocumentRetriever\n",
    ")\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Ragas (0.2.10) - \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ContextPrecision, ContextRecall, AnswerRelevancy, ContextEntityRecall\n",
    "from ragas.testset import TestsetGenerator\n",
    "from datasets import Dataset  # For Ragas 0.2.10 evaluation format\n",
    "\n",
    "# LangSmith\n",
    "from langsmith import Client\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys configured and LangSmith tracing enabled\n"
     ]
    }
   ],
   "source": [
    "# API Keys Setup\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API Key: \")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass(\"Enter Cohere API Key: \")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Enter LangChain/LangSmith API Key: \")\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Advanced_Retrieval_Evaluation\"\n",
    "\n",
    "langsmith_client = Client()\n",
    "print(\"‚úÖ API keys configured and LangSmith tracing enabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading PDF: data/TC.pdf\n",
      "‚úÖ Loaded 6 pages from PDF\n",
      "   First page preview: Traffic Congestion and Reliability: Trends, \n",
      "Causes, Measurement, and Strategic \n",
      "Approaches \n",
      "Traffic congestion is a pervasive issue that impacts economic efficiency, quality of life, and \n",
      "overall tra...\n",
      "‚úÖ Split into 18 chunks for retrieval\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Load PDF\n",
    "pdf_path = \"data/TC.pdf\"\n",
    "print(f\"üìÑ Loading PDF: {pdf_path}\")\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} pages from PDF\")\n",
    "print(f\"   First page preview: {documents[0].page_content[:200]}...\")\n",
    "\n",
    "# Split for retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"‚úÖ Split into {len(chunks)} chunks for retrieval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9a3ca89ac14e829d384b0ec5d5d4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4cdb050116472884db90eea137b4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e141b962ed44501b27286295a2aa8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64a5e1235964968a1a9b5e441d64150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c0a2d8577d40f4ad1eb828b67bcdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d55692d2ed439a86c44529f0e38780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3179dd8d10c4ef38c0bdf56447fe871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 10 synthetic questions!\n",
      "\\nüìã Sample questions:\n",
      "   1. What role do traffic incidents play in causing traffic congestion?\n",
      "   2. Wut are the effects of special evnts on traffic demand?\n",
      "   3. How does congestion at border crossings affect international trade?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role do traffic incidents play in causing...</td>\n",
       "      <td>[Traffic Congestion and Reliability: Trends, \\...</td>\n",
       "      <td>Traffic incidents, such as crashes, breakdowns...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wut are the effects of special evnts on traffi...</td>\n",
       "      <td>[speeds and increasing delays. These disruptio...</td>\n",
       "      <td>Special events, such as major sporting contest...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does congestion at border crossings affect...</td>\n",
       "      <td>[Travel time is the fundamental metric for con...</td>\n",
       "      <td>Congestion at border crossings elevates costs ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can travler informashun help in reducing c...</td>\n",
       "      <td>[Households face both financial and time-relat...</td>\n",
       "      <td>Traveler information provides real-time data o...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are ITS technolgies used for in traffic m...</td>\n",
       "      <td>[efficiently not only restores traffic flow mo...</td>\n",
       "      <td>ITS technologies are integrated into comprehen...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the primary causes of traffic congest...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic congestion is primarily caused by a co...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the causes of traffic congestion and ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic congestion is caused by multiple inter...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the causes of traffic congestion and ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic congestion occurs when the number of v...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do traffic incidents contribute to traffic...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic incidents, such as crashes, breakdowns...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the primary causes of traffic congest...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nefficiently not only restores traf...</td>\n",
       "      <td>Traffic congestion is primarily caused by a co...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What role do traffic incidents play in causing...   \n",
       "1  Wut are the effects of special evnts on traffi...   \n",
       "2  How does congestion at border crossings affect...   \n",
       "3  How can travler informashun help in reducing c...   \n",
       "4  What are ITS technolgies used for in traffic m...   \n",
       "5  What are the primary causes of traffic congest...   \n",
       "6  What are the causes of traffic congestion and ...   \n",
       "7  What are the causes of traffic congestion and ...   \n",
       "8  How do traffic incidents contribute to traffic...   \n",
       "9  What are the primary causes of traffic congest...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Traffic Congestion and Reliability: Trends, \\...   \n",
       "1  [speeds and increasing delays. These disruptio...   \n",
       "2  [Travel time is the fundamental metric for con...   \n",
       "3  [Households face both financial and time-relat...   \n",
       "4  [efficiently not only restores traffic flow mo...   \n",
       "5  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "6  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "7  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "8  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "9  [<1-hop>\\n\\nefficiently not only restores traf...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Traffic incidents, such as crashes, breakdowns...   \n",
       "1  Special events, such as major sporting contest...   \n",
       "2  Congestion at border crossings elevates costs ...   \n",
       "3  Traveler information provides real-time data o...   \n",
       "4  ITS technologies are integrated into comprehen...   \n",
       "5  Traffic congestion is primarily caused by a co...   \n",
       "6  Traffic congestion is caused by multiple inter...   \n",
       "7  Traffic congestion occurs when the number of v...   \n",
       "8  Traffic incidents, such as crashes, breakdowns...   \n",
       "9  Traffic congestion is primarily caused by a co...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_specific_query_synthesizer  \n",
       "7  multi_hop_specific_query_synthesizer  \n",
       "8  multi_hop_specific_query_synthesizer  \n",
       "9  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3: Golden Dataset Generation (Ragas SDG 0.2.10) - 10 Questions\n",
    "\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "golden_dataset = generator.generate_with_langchain_docs(documents, testset_size=10)\n",
    "\n",
    "# Convert to pandas for easy access\n",
    "golden_df = golden_dataset.to_pandas()\n",
    "\n",
    "print(f\"‚úÖ Generated {len(golden_df)} synthetic questions!\")\n",
    "print(\"\\\\nüìã Sample questions:\")\n",
    "for i in range(min(3, len(golden_df))):\n",
    "    print(f\"   {i+1}. {golden_df.iloc[i]['user_input']}\")\n",
    "\n",
    "# Display full dataset\n",
    "golden_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All 6 retrievers ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/blff0y8x70d7sdk3bw6bzkvc0000gn/T/ipykernel_80773/3072977747.py:27: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
      "  base_compressor=CohereRerank(model=\"rerank-english-v3.0\", top_n=5),\n"
     ]
    }
   ],
   "source": [
    "# Section 4: Setup All 6 Retrievers\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 1. Naive\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "naive = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 2. BM25\n",
    "bm25 = BM25Retriever.from_documents(chunks); bm25.k = 5\n",
    "\n",
    "# 3. Multi-Query\n",
    "multi_query = MultiQueryRetriever.from_llm(retriever=naive, llm=llm)\n",
    "\n",
    "# 4. Parent Document\n",
    "store = InMemoryStore()\n",
    "parent_doc = ParentDocumentRetriever(\n",
    "    vectorstore=FAISS.from_documents(RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50).split_documents(documents), embeddings),\n",
    "    docstore=store,\n",
    "    child_splitter=RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50),\n",
    "    parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    ")\n",
    "parent_doc.add_documents(documents)\n",
    "\n",
    "# 5. Contextual Compression (Cohere Rerank)\n",
    "compression = ContextualCompressionRetriever(\n",
    "    base_compressor=CohereRerank(model=\"rerank-english-v3.0\", top_n=5),\n",
    "    base_retriever=naive\n",
    ")\n",
    "\n",
    "# 6. Ensemble\n",
    "ensemble = EnsembleRetriever(retrievers=[bm25, naive, multi_query], weights=[0.3, 0.4, 0.3])\n",
    "\n",
    "retrievers = {\"Naive\": naive, \"BM25\": bm25, \"Multi-Query\": multi_query, \n",
    "              \"Parent Document\": parent_doc, \"Contextual Compression\": compression, \"Ensemble\": ensemble}\n",
    "\n",
    "print(f\"‚úÖ All {len(retrievers)} retrievers ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 6 RAG chains ready with LangSmith tracking\n"
     ]
    }
   ],
   "source": [
    "# Section 5: Build RAG Chains with LangSmith Tracking\n",
    "RAG_TEMPLATE = \"\"\"You are a helpful AI assistant. Use the context to answer the question.\n",
    "If you don't know, say so. Don't make up information.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "rag_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def build_chain(retriever):\n",
    "    return (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=lambda x: \"\\\\n\\\\n\".join(d.page_content for d in x[\"context\"]))\n",
    "        | {\"response\": rag_prompt | rag_llm, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "\n",
    "rag_chains = {name: build_chain(r) for name, r in retrievers.items()}\n",
    "print(f\"‚úÖ {len(rag_chains)} RAG chains ready with LangSmith tracking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating 6 retrievers with 5 Ragas metrics...\n",
      "\\n==================================================\\nEvaluating: Naive\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6af2a9fe994a06ac959b06beabfd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Complete (122.9s): {'context_precision': '1.0000', 'context_recall': '0.9250', 'answer_relevancy': '0.8696', 'faithfulness': '0.9537', 'context_entity_recall': '0.3681'}\n",
      "\\n==================================================\\nEvaluating: BM25\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c97789c3b824416b4b27a92202b95d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Complete (110.1s): {'context_precision': '1.0000', 'context_recall': '0.8000', 'answer_relevancy': '0.8773', 'faithfulness': '0.9329', 'context_entity_recall': '0.2899'}\n",
      "\\n==================================================\\nEvaluating: Multi-Query\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acae05074423405c83c91bdad076887f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Complete (149.3s): {'context_precision': '1.0000', 'context_recall': '0.9167', 'answer_relevancy': '0.8697', 'faithfulness': '0.9758', 'context_entity_recall': '0.4250'}\n",
      "\\n==================================================\\nEvaluating: Parent Document\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4800800863294042a27be7d0e7ab102d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Complete (124.4s): {'context_precision': '1.0000', 'context_recall': '0.8250', 'answer_relevancy': '0.8697', 'faithfulness': '0.8739', 'context_entity_recall': '0.2947'}\n",
      "\\n==================================================\\nEvaluating: Contextual Compression\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25da27174c9143a38d312ef3c73133bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Complete (128.3s): {'context_precision': '1.0000', 'context_recall': '0.8917', 'answer_relevancy': '0.8695', 'faithfulness': '0.9446', 'context_entity_recall': '0.3136'}\n",
      "\\n==================================================\\nEvaluating: Ensemble\\n==================================================\n",
      "  Question 1/10...\\r  Question 2/10...\\r  Question 3/10...\\r  Question 4/10...\\r  Question 5/10...\\r  Question 6/10...\\r  Question 7/10...\\r  Question 8/10...\\r  Question 9/10...\\r  Question 10/10...\\r\\n  Running Ragas evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3937bacb1c4895954c6bcab845896b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Complete (173.5s): {'context_precision': '1.0000', 'context_recall': '0.9500', 'answer_relevancy': '0.8685', 'faithfulness': '0.9800', 'context_entity_recall': '0.3349'}\n",
      "\\n======================================================================\n",
      "üìä EVALUATION COMPLETE - QUICK SUMMARY\n",
      "======================================================================\n",
      "             Retriever  Questions Latency (s)\n",
      "                 Naive         10       122.9\n",
      "                  BM25         10       110.1\n",
      "           Multi-Query         10       149.3\n",
      "       Parent Document         10       124.4\n",
      "Contextual Compression         10       128.3\n",
      "              Ensemble         10       173.5\n",
      "\\n‚úÖ Full detailed results in next cell...\n"
     ]
    }
   ],
   "source": [
    "# Section 6: Evaluate All Retrievers with Ragas Metrics (0.2.10 API)\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "# Initialize metrics with LLM and embeddings\n",
    "metrics = [\n",
    "    ContextPrecision(llm=evaluator_llm), \n",
    "    ContextRecall(llm=evaluator_llm), \n",
    "    AnswerRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings), \n",
    "    Faithfulness(llm=evaluator_llm), \n",
    "    ContextEntityRecall(llm=evaluator_llm)\n",
    "]\n",
    "\n",
    "print(f\"üìä Evaluating {len(retrievers)} retrievers with {len(metrics)} Ragas metrics...\")\n",
    "evaluation_results = {}\n",
    "\n",
    "for name, chain in rag_chains.items():\n",
    "    print(f\"\\\\n{'='*50}\\\\nEvaluating: {name}\\\\n{'='*50}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Build evaluation data (Ragas 0.2.10 format)\n",
    "    # Required columns: question, answer, contexts, ground_truths, reference\n",
    "    eval_data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truths\": [], \"reference\": []}\n",
    "    \n",
    "    # Use enumerate to get proper sequential count\n",
    "    for idx in range(len(golden_df)):\n",
    "        row = golden_df.iloc[idx]\n",
    "        print(f\"  Question {idx+1}/{len(golden_df)}...\", end=\"\\\\r\")\n",
    "        try:\n",
    "            question = row['user_input']\n",
    "            reference = row.get('reference', '')\n",
    "            \n",
    "            result = chain.invoke({\"question\": question})\n",
    "            \n",
    "            eval_data[\"question\"].append(question)\n",
    "            eval_data[\"answer\"].append(result[\"response\"].content)\n",
    "            eval_data[\"contexts\"].append([result[\"context\"]])\n",
    "            eval_data[\"ground_truths\"].append([reference] if reference else [question])\n",
    "            eval_data[\"reference\"].append(reference if reference else question)\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "        except Exception as e:\n",
    "            print(f\"\\\\n  ‚úó Error on question {idx+1}: {e}\")\n",
    "            continue  # Skip failed questions\n",
    "    \n",
    "    print(f\"\\\\n  Running Ragas evaluation...\")\n",
    "    from datasets import Dataset\n",
    "    eval_dataset = Dataset.from_dict(eval_data)\n",
    "    ragas_results = evaluate(eval_dataset, metrics=metrics, llm=evaluator_llm)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Extract metric scores for display\n",
    "    metric_scores = {}\n",
    "    for metric in metrics:\n",
    "        if hasattr(ragas_results, metric.name):\n",
    "            metric_scores[metric.name] = f\"{getattr(ragas_results, metric.name):.4f}\"\n",
    "    \n",
    "    evaluation_results[name] = {\"ragas\": ragas_results, \"latency\": elapsed, \"count\": len(eval_data[\"question\"])}\n",
    "    print(f\"  ‚úÖ Complete ({elapsed:.1f}s): {metric_scores}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"üìä EVALUATION COMPLETE - QUICK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Quick summary table\n",
    "summary_data = []\n",
    "for name, res in evaluation_results.items():\n",
    "    ragas_result = res[\"ragas\"]\n",
    "    row = {\n",
    "        \"Retriever\": name,\n",
    "        \"Questions\": res[\"count\"],\n",
    "        \"Latency (s)\": f\"{res['latency']:.1f}\"\n",
    "    }\n",
    "    # Add first few metrics for quick view\n",
    "    for metric in metrics[:3]:  # Show first 3 metrics\n",
    "        if hasattr(ragas_result, metric.name):\n",
    "            row[metric.name] = f\"{getattr(ragas_result, metric.name):.3f}\"\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\\\n‚úÖ Full detailed results in next cell...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüìä FINAL RESULTS:\n",
      "             Retriever  Latency (s)  Questions\n",
      "                  BM25   110.129075         10\n",
      "                 Naive   122.944868         10\n",
      "       Parent Document   124.400105         10\n",
      "Contextual Compression   128.307050         10\n",
      "           Multi-Query   149.312663         10\n",
      "              Ensemble   173.492774         10\n",
      "\\n‚úÖ Saved to: retriever_evaluation_results.csv\n",
      "\\nüèÜ Best by Metric:\n",
      "\\nüìù RECOMMENDATION:\n",
      "\n",
      "Based on cost, latency, and performance:\n",
      "- For best performance: Check scores above\n",
      "- For best speed: Check latency column  \n",
      "- For balanced approach: Consider all factors\n",
      "\n",
      "\\nüîó LangSmith Links:\n",
      "   üìä Project Dashboard: https://smith.langchain.com/o/default/projects/p/advanced-retrieval-evaluation\n",
      "   üîç View All Traces: https://smith.langchain.com/\n",
      "   üìÅ Project: Advanced_Retrieval_Evaluation\n",
      "\\n   üí° Tip: Use the LangSmith dashboard to:\n",
      "      - View detailed traces for each retriever\n",
      "      - Compare latency and token usage\n",
      "      - Debug retrieval quality issues\n",
      "      - Analyze cost breakdowns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever</th>\n",
       "      <th>Latency (s)</th>\n",
       "      <th>Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25</td>\n",
       "      <td>110.129075</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive</td>\n",
       "      <td>122.944868</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parent Document</td>\n",
       "      <td>124.400105</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contextual Compression</td>\n",
       "      <td>128.307050</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multi-Query</td>\n",
       "      <td>149.312663</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>173.492774</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Retriever  Latency (s)  Questions\n",
       "1                    BM25   110.129075         10\n",
       "0                   Naive   122.944868         10\n",
       "3         Parent Document   124.400105         10\n",
       "4  Contextual Compression   128.307050         10\n",
       "2             Multi-Query   149.312663         10\n",
       "5                Ensemble   173.492774         10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 7: Results Compilation and Analysis\n",
    "results_data = []\n",
    "for name, res in evaluation_results.items():\n",
    "    # Ragas 0.2.10: EvaluationResult has metric scores as attributes\n",
    "    ragas_result = res[\"ragas\"]\n",
    "    row = {\n",
    "        \"Retriever\": name, \n",
    "        \"Latency (s)\": res[\"latency\"], \n",
    "        \"Questions\": res[\"count\"]\n",
    "    }\n",
    "    \n",
    "    # Extract metric scores from EvaluationResult\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.name\n",
    "        if hasattr(ragas_result, metric_name):\n",
    "            row[metric_name] = getattr(ragas_result, metric_name)\n",
    "    \n",
    "    results_data.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results_data).sort_values(\"Latency (s)\")\n",
    "print(\"\\\\nüìä FINAL RESULTS:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(\"retriever_evaluation_results.csv\", index=False)\n",
    "print(\"\\\\n‚úÖ Saved to: retriever_evaluation_results.csv\")\n",
    "\n",
    "# Analysis\n",
    "best = {col: results_df.loc[results_df[col].idxmax(), \"Retriever\"] \n",
    "        for col in results_df.columns if col not in [\"Retriever\", \"Latency (s)\", \"Questions\"]}\n",
    "\n",
    "print(\"\\\\nüèÜ Best by Metric:\")\n",
    "for metric, retriever in best.items():\n",
    "    print(f\"   {metric}: {retriever}\")\n",
    "\n",
    "print(\"\\\\nüìù RECOMMENDATION:\")\n",
    "print(\"\"\"\n",
    "Based on cost, latency, and performance:\n",
    "- For best performance: Check scores above\n",
    "- For best speed: Check latency column  \n",
    "- For balanced approach: Consider all factors\n",
    "\"\"\")\n",
    "\n",
    "# LangSmith URLs\n",
    "project_name = \"Advanced_Retrieval_Evaluation\"\n",
    "print(\"\\\\nüîó LangSmith Links:\")\n",
    "print(f\"   üìä Project Dashboard: https://smith.langchain.com/o/default/projects/p/{project_name.replace('_', '-').lower()}\")\n",
    "print(f\"   üîç View All Traces: https://smith.langchain.com/\")\n",
    "print(f\"   üìÅ Project: {project_name}\")\n",
    "print(\"\\\\n   üí° Tip: Use the LangSmith dashboard to:\")\n",
    "print(\"      - View detailed traces for each retriever\")\n",
    "print(\"      - Compare latency and token usage\")\n",
    "print(\"      - Debug retrieval quality issues\")\n",
    "print(\"      - Analyze cost breakdowns\")\n",
    "\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
